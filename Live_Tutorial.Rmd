---
author: "Killian, Luca & Aranxa"
title: "Quanteda"
subtitle: "Quantitative Text Analyis"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Welcome!👋

This Workshop introduces you to quanteda, an R package for quantitative text analysis.

# Text Analysis 📚

Why do we want to analyze textual data? Text is everywhere, grows in volume and becomes more easily accessible as we speak. Main reasons for this are the widespread use of
social media and the digitalisation of many aspects of our lives, making lots of data
available for (social science) research. Natural language processing provides us with
the fundamental tools and methods to analyse text data.

### Natural Language Processing
Natural Language Processing (NLP) is concerned with enabling computers/machines to
process, understand, interpret and even generate natural language, for example in the
form of text or speech. Some examples of applications using NLP are chatbots, tools for
speech recognition,  automatic text summarization or translation, such as Google
Translate.

### Quantitative Text Analysis
Quantitative text analysis is a subfield of NLP and refers to the process of analyzing
text data using statistical and computational methods to derive quantitative information
from the input text. Some examples of quantitative text analysis include word frequency
analysis, keyword extraction, sentiment analysis, text visualization, and more. All of this can be done using the R package quanteda!

### Use Cases
Some popular use cases of quantitative text analysis in social science research are:
-   The analysis of political speeches or party manifestos to identify key phrases
-   Sentiment analysis of social media posts
-   Categorization and quantification of open-ended survey responses
In order to illustrate the workflow and main functions of the quanteda package, we
analyse speeches by Barack Obama which we previously scraped from (insert link). (TO DO:
add more explanation about data?)


# Quanteda 💡

### Background
Quanteda is an R packagae for managing and analysing textual data. It was created by
[Ken Benoit](https://kenbenoit.net) and is available via [CRAN](https://cran.r-
project.org/web/packages/quanteda/index.html). The latest update of quanteda is version 4.0 and the package is maintained by the UK-based non-profit company [Quanteda
Initiative](https://quanteda.org).

Since Version 3.0, quanteda has been split into modular packages. These consist of:
**quanteda:**

Contains all core natural language processing and textual data management functions

**quanteda.textmodels:**
-   Contains all text models and supporting functions
-   Textmodel_* functions

**quanteda.textstats:**
-   Statistics for textual data
-   textstat_*() functions

**quanteda.textplots:**
-   Plots for textual data
-   textplot_*() functions

Additionally available via GitHub:

**quanteda.sentiment:**
-   Sentiment analysis using dictionaries

**quanteda.tidy:**
-   Extensions for manipulating document variables in core quanteda objects using
tidyverse functions

# Quanteda Basics 📝

The quanteda workflow is structured around three main objects, these are

1. the Corpus
2. Tokens
3. The Document-Feature Matrix (dfm)

## Corpus
In quanteda, a corpus functions as the primary data structure for storing and organizing
text data. The corpus can be thought of as “library" that holds the textual data we want
to analyse alongside specific document-level or metadata attributes, called docvars.
Only by transforming our input text into a corpus format, quanteda is able to process
it, therefore the corpus functions as starting point to our analysis. In general, the
corpus functions as relatively static library, meaning that text data in the corpus
should not be changed through cleaning and preprocessing steps. Rather, texts can be
extracted from the corpus for any manipulation and should be assigned to new objects.
This way, the corpus functions as a copy of the original input data.
Within the corpus, each document is typically represented as a separate element.
Documents within a corpus can be accessed by their index or via docvars.

## Tokens
In quanteda, tokens represent the basic units of text data, that are used for any
further analysis. They comprise usually of words that have been extracted and grouped as
semantic units. However, phrases or even entire sentences can be tokenized as well.
Tokenizing describes the process of splitting the input text into individual tokens.
Once a text is tokenized, various operations for preprocessing and cleaning and feature
extraction can be performed on those tokens.
**How do tokens relate to the Corpus?**
When you tokenize text, you create a list of tokens for each document, and each document
in the corpus is represented as a sequence of tokens.

## Document Feature Matrix
The document feature matrix puts documents into matrix format and constitutes the unit
on which analyses will be performed. In the DFM, each row corresponds to a document and
each column corresponds to a feature of that text, these are often tokens. The matrix
stores the frequency of each feature (column) within each document (row). DFMs are
particularly useful if not all parts of the documents in the corpus should be included
in the analysis.

DFMs are often sparse, meaning that many entries in the matrix are zero, making the DFM
more memory- and computationally efficient.
#TO DO: Explain better what 'features' entails --> Luca
**How does the DFM relate to tokens?**
When you create DFM, the list of tokens is transformed into a structure matrix that
functions as foundation for subsequent analysis, such as word frequency analysis,
sentiment analysis, text classification, etc. Tokens constitute individual elements of
text data, while a DFM constitutes a structural and numerical representation of text
data.

# Why quanteda? ✅

There are several advantages to using quanteda for quantitative text analysis!

-   Quanteda is compatible with other R packages, for example the magrittr |> and most
of the grammar of the tidyverse!
-   Quanteda is a well maintained package: The quanteda initiative offers user,
technical and development support as well as teaching and workshop opportunities
-   It is a relatively easy to use for beginners but offers complex functions for more
advanced analyses too
-   And it is fast: Compared to other R and Python package for processinf large textual
data, quanteda is faster and more efficient


...And now it's time to try it yourself!

---
# Workflow 💪

# Loading the package
First we load the core of the quanteda package.

If any of the packages are not installed on your machine you can uncomment the first line.

```{r initialise, warning=FALSE}
#install.packages('quanteda','quanteda.textstats','readtext','gt')

library(quanteda)
```

As mentioned above, the quanteda package is split in to modular packages so we also download a selection of those of interest to us in this tutorial.

```{r initialise2, warning=FALSE}
library(quanteda.textstats)
library(quanteda.textplots)
```

We also load readtext which makes the process of importing texts easier, and also imports meta data. Specifically in our case, it allows us to read in pdfs as text files.
Other compatible input formats include .txt, .csv, .tab, .json, .doc and
.docx files.

```{r initialise3, warning=FALSE}
library(readtext)
```

Finally we load dplyr for our workflow and gt for creating nice looking tables. 

```{r initialise4, warning=FALSE}
library(dplyr)
library(gt)
```

# Loading and prepping our data - The Corpus

For the example in this tutorial we will load in a collection of slides from the Intro to Data Science Course by Simon Munzert. We use the readtext function as mentioned above.

```{r}
ids_presentations_df = readtext("./Slideshows/*.pdf", docvarsfrom = "filenames")
#rt_pdf = readtext("./Slideshows/*.pdf", docvarsfrom = "filenames", dvsep = "-")
```

## Adding your own slideshow

We encourage you to add your own slideshow in pdf format. To do this, just insert the pdf file in the Input folder and delete any other contents. For demonstration purposes we have included the slideshow you just one, but we think that this demonstration will be more interesting and fun if you can add your own data.

We will then add the text and info on the presentation such as it's title to our presentations data frame. This is straightforward as readtext will give us the same variable names for both imports. We also add an indicator variable which tells us which presentation is the 'extra' one we added. This will be important later for some of the analysis we want to do.

```{r}
your_presentation_df = readtext("./Import/*.pdf", docvarsfrom = "filenames")
if (nrow(your_presentation_df) > 1) stop("More than one pdf in Import folder, please remove extras") 

your_presentation_df$yourpresentation = c(TRUE)
ids_presentations_df$yourpresentation = rep(FALSE,rep=nrow(ids_presentations_df))
#These two lines create the variable your presentation and set it to true only for the presentation imported from the Import folder

yourpresentationname = as.character(as.data.frame(your_presentation_df)[[1]])
#this line saves the name of your specific presentation for later use

presentations_df = rbind(ids_presentations_df, your_presentation_df)
print(presentations_df, n = 10)
```

Here we can see the structure of our dataframe. Luckily for us, readtext automatically formats the dataframe as quantedata expects them, but for future reference if you are constructing your own dataframes for analysis, the text for analysis for each row (which will be each unique text, presentation, section or any other unit of analysis) must be in a column called 'text'.

## Creating a  Corpus

We can now create our first Corpus - this is the object at the centre of the quanteda package.  

```{r}
presentations_corpus = corpus(presentations_df)
summary(presentations_corpus)
```

Here we see the corpus structure with some information about the text in each slideshow, plus the document-level variables we added earlier.

# Preparing our texts for non-relational analysis - Tokens and the DFM

Now that we have our corpus we can begin to analyse the text data. For this section we will focus on non-relational analysis, i.e. we will look at the presence and frequency of words in each slideshow, regardless of their context. The first process our data will undergo is tokenisation. This splits up the text of each document into tokens, which are usually words, but can be other standalone character units.

Once we have tokenised our corpus we can create a Document feature matrix as introduced above. We introduce these concepts these concepts together as that is how you will usually emply the min the wild. But there are also some use cases where you might stop after tokenisation.

Finally, we run the function top features, to have a look at the most common tokens in the entire corpus. This is our first analysis function, just to get an overview, but we will dive into more later.

Don't worry if this process of tokenisation and constructing a dfm still feels unclear, we will go over it a few times, and you'll have a chance to adjust the process yourselves.

```{r}
presentation_tokens = tokens(presentations_corpus)

presentation_dfm = dfm(presentation_tokens)

print(presentation_dfm)

topfeatures(presentation_dfm)
```

There we have it, our first DFM! Not so difficult to begin with.

Looking at the top features, we can see that all the most common tokens by far are punctuation or filler words which don't carry much meaning. This is normal, and luckily quantedata comes built-in with some handy functions and options to help alleviate this.

## Cleaning our Tokens

The first thing we can do is adjust the parameters on the function tokens(), allowing us to remove punctuation, numbers and symbols.

An even more powerful tool is the token_remove function combined with the built-in stopwords database. Quantedata comes loaded with common filler words or stop words for _ different languages. Removing these may help us with our analysis.

```{r}
presentation_tokens = tokens(presentations_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)

presentation_tokens = tokens_remove(presentation_tokens, stopwords("en"))

presentation_dfm = dfm(presentation_tokens)

topfeatures(presentation_dfm)
```
Now this looks more promising!

### Bringing it all together

You can use your own discretion to decide what exactly should be kept or excluded from your tokens. For example, I felt that 'can' and 'use' were not words that carried much useful meaning, especially with their very high frequency in the data, so I choose to remove the mbelow. Have a look at the outputs above and see if there are any adjustments you would make for our final tokenisation process, then apply them below.

We have included some functions for you already here and there are even more in the section entitled: More Token Cleaunup, whcih you can add to the pipeline.

(We repeat the whole process here for the sake of having it all in once place as an overview)

```{r}
presentation_tokens = tokens(presentations_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,remove_url = TRUE, remove_separators = TRUE, split_hyphens = FALSE, split_tags = FALSE)#Do you think we should be excluding all these categories? Feel free to change these options as you feel makes the most sense

presentation_dfm = presentation_tokens %>% 
  tokens_remove(pattern= stopwords("en")) %>% #removing the stopwords as before
  tokens_remove(pattern = c('can','use')) %>% #Removing can and use as mentioned, you can add any 'filler' words which you would like excluded from the analysis here.
  #Some more functions you may want to consider for cleaning your tokens (uncomment first line for each to use and remember to include pipe symbol at the end):
  #tokens_tolower() %>%
  #Changes all tokens to be lower case
  #tokens_toupper() %>%
  #Changes all tokens to be upper case
  #tokens_replace(pattern = c('color'), replacement = ('colour')) %>%
  #Allows you to find and replace words in your tokens, remember that pattern and replacement should always be the same length and that entries are matched pairwise.
  dfm() #Finally we run the dfm function on our finished and cleaned tokens.

topfeatures(presentation_dfm)
```

Now we have a nice looking Document Feature matrix for our further analysis. For our live presentation we will stop here and move on to the next section: Analysing text non-relationally, but if you would like to learn some more about creating and tweaking corpuses continue below.

## Additional Functions for Corpuses

Subset corpora:
```{r}corpus_subset()```
```{r}corpus_subset(x, Year > 1990)```

Change units of a corpus:
```{r}corpus_reshape()```
```{r}corpus_reshape(x, to = c("sentences", "paragraphs"))```

Segment texts on a pattern match:
```{r}corpus_segment()```
```{r}corpus_segment(x, pattern, valuetype, extract_pattern = TRUE)```

Take a random sample of the corpus texts:
```{r}corpus_sample```
```{r}corpus_sample(x, size = 10, replace = FALSE)```

## Additional Funtions for Tokens

### More Cleaning

tokens_keep() - For when we only want to keep tokens matching a certain criteria.

Tokens_select() - The function underlying tokens_keep and tokens_remove. Use the selections = 'keep' or 'remove' parameter to pick what you want to do with the matched tokens.

Tokens_wordstem() - This is a really interesting and useful function that reduces all words and tokens to their root. We use a very simple sentence below to show an example of this in action.

```{r}
tokens_wordstem(tokens('She runs, he can run, they are running'))
```

```{r}Tokens_compound()```

### Other Token functions (From Luca's list, still need doing.)
```{r}Tokens_lookup()```

```{r}Tokens_ngrams()```

```{r}Tokens_skipgrams()```

```{r}Tokens_sample()```

```{r}Tokens_subset()```

### More ocument Feature Matrix functions
```dictionary()```

```dfm_lookup()```

```dfm_select()```

```dfm_compress()```

```dfm_sample()```

```dfm_weight()```

```dfm_sort()```

# Analysing our texts non-relationally

There are many many functions across the quantedata family of packages which can be used to analyse our newly created DFM, here we will focus on some of the fucntions we foudn the most interesting from the textstat and testplot packages.

## Lexical Diversity

The first function we look at is textstat_lexdiv(), which calculates the lexical diverstiy of the text. As a default this is the ratio of unique tokens to total tokens in the text (TTR), but you can also use the (measure = ) parameter to change to measure.

Also important to note here that while the output of most functions in textstat look like data frames, they are initially stored as lists, of specific type depending on the funciton. We use as.list(), or as.data.frame() to tranform them into more useable data types as seen below.

```{r}
typeof(textstat_lexdiv(presentation_dfm))

as.data.frame(textstat_lexdiv(presentation_dfm)) %>%
  arrange(desc(TTR)) %>% gt() #ordering our results in terms of TTR and presenting them in a nice table
```

## Distance and Similarity

Another interesting function for looking at the distance between two text documents is textstat_dist(), which can calculate the distance between two documents, which can most easily be understood as how different the choice and frequency of words used is between the documents. 

```{r}
as.data.frame(textstat_dist(presentation_dfm)) %>% #Here we run the function, with the default, euclidean distance measurement.
  
  filter(document2 == yourpresentationname) %>% arrange(euclidean) %>% select (c(1,3)) %>% gt() #Here we use dplyr to filter for only the distances relative to your presentation, arrange in ascending order of distance and filter out the column of your presentation name repeated and gt to give us a nice readable table
```

Which IDS slideshow is your presentation most like?

Another similar function is textstat_simil() which calculates similarities, and defaults to the correlation in two documents. See if you can make a similar function to the one above and see how well the results match.

```{r}
#Insert your code here:
```

## Grouping DFMs

Above, we filtered out all comparisons not involving your presentation manually, but because our document-level variables have been retained in our DFM, we can actually group the presentation by the variable 'yourpresentation' we created earlier. (Remember, this variable is true for the presentation you inputted and false for all others.)

One way of doing this is the dfm_group() function, which is useful as part of a pipe, to pass the group DFMs based on a parameter of interest into the next function.

Some functions in the package have funcitonality for this built in, as we can see with topfeatures below.

```{r}
dfm_group(presentation_dfm, presentation_dfm$yourpresentation)

topfeatures(presentation_dfm, groups = yourpresentation)
```

While this is just a toy example as our documents don't have many document level variables, there are many useful applications of this in analysing texts, like grouping by chapter in a book, by which party gave a political speech, or any other category of interest in our documents.


## Word Clouds

Just for fun: lets make an ever popular word cloud! We can do this super easily with the textplot_wordcloud() function.

```{r}
textplot_wordcloud(presentation_dfm, max_words = 50)

#dfm_group(presentation_dfm, presentation_dfm$yourpresentation) %>% textplot_wordcloud( max_words = 20, comparison = TRUE) - Can't get working yet - Killian
```


# Natural Language Processing

