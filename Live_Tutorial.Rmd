---
author: "Killian, Luca & Aranxa"
title: "Quanteda"
subtitle: "Quantitative Text Analyis"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Welcome!ðŸ‘‹

This Workshop introduces you to quanteda, an R package for quantitative text analysis.

# Text Analysis ðŸ“š

Why do we want to analyze textual data? Text is everywhere, grows in volume and becomes more easily accessible as we speak. Main reasons for this are the widespread use of
social media and the digitalisation of many aspects of our lives, making lots of data
available for (social science) research. Natural language processing provides us with
the fundamental tools and methods to analyse text data.

### Natural Language Processing
Natural Language Processing (NLP) is concerned with enabling computers/machines to
process, understand, interpret and even generate natural language, for example in the
form of text or speech. Some examples of applications using NLP are chatbots, tools for
speech recognition,  automatic text summarization or translation, such as Google
Translate.

### Quantitative Text Analysis
Quantitative text analysis is a subfield of NLP and refers to the process of analyzing
text data using statistical and computational methods to derive quantitative information
from the input text. Some examples of quantitative text analysis include word frequency
analysis, keyword extraction, sentiment analysis, text visualization, and more. All of
this can be done using the R package quanteda!

### Use Cases
Some popular use cases of quantitative text analysis in social science research are:
-   The analysis of political speeches or party manifestos to identify key phrases
-   Sentiment analysis of social media posts
-   Categorization and quantification of open-ended survey responses
In order to illustrate the workflow and main functions of the quanteda package, we
analyse speeches by Barack Obama which we previously scraped from (insert link). (TO DO:
add more explanation about data?)


# Quanteda ðŸ’¡

### Background
### Background
Quanteda is an R packagae for managing and analysing textual data. It was created by
[Ken Benoit](https://kenbenoit.net) and is available via [CRAN](https://cran.r-
project.org/web/packages/quanteda/index.html). The latest update of quanteda is version 4.0 and the package is maintained by the UK-based non-profit company [Quanteda
Initiative](https://quanteda.org).

Since Version 3.0, quanteda has been split into modular packages. These consist of:
**quanteda:**

Contains all core natural language processing and textual data management functions

**quanteda.textmodels:**
-   Contains all text models and supporting functions
-   Textmodel_* functions

**quanteda.textstats:**
-   Statistics for textual data
-   textstat_*() functions

**quanteda.textplots:**
-   Plots for textual data
-   textplot_*() functions

Additionally available via GitHub:
**quanteda.sentiment:**
-   Sentiment analysis using dictionaries

**quanteda.tidy:**
-   Extensions for manipulating document variables in core quanteda objects using
tidyverse functions

# Quanteda Basics ðŸ“

### Main Objects

#### Corpus 

#### Tokens 

#### Document Feature Matrix 


# Workflow ðŸ’ª


#### Getting started 

First we load the core quanteda package.
```{r}
#install.packages('quanteda')
library(quanteda)
```

Additionally, we load the [readtext package](https://readtext.quanteda.io)

```{r}
#install.packages('readtext')
library(readtext)
```
The readtext package provides an easy way to read text data into R, from almost any input format. 

```{r}
#install.packages('dplyr')
library(dplyr)
#install.packages('gt')
library(gt)
```
These are some extra packages that are nice to have.

#### Main Functions



# Why quanteda? âœ…


---
# Loading the package
If 

First we load the core of the quanteda package. If any of the packages are not installed on your machine you can uncomment the first line.

```{r initialise, warning=FALSE}
#install.packages('quanteda','quanteda.textstats','readtext')

library(quanteda)
```

The package is split in to modular packages so we also download a selection of those of interest to us in this tutorial.

```{r initialise2, warning=FALSE}
library(quanteda.textstats)
library(quanteda.textplots)
```

Finally we load readtext which makes the process of importing texts easier, and also imports meta data. Specifically in our case, it allows us to read in pdfs as text files.

```{r initialise3, warning=FALSE}
library(readtext)
```

# Loading and prepping our data - The Corpus

For the example in this tutorial we will load in a collection of slides from the Intro to Data Science Course by Simon Munzert. We use the readtext function as mentioned above.

```{r}
ids_presentations_df = readtext("./Slideshows/*.pdf", docvarsfrom = "filenames")
#rt_pdf = readtext("./Slideshows/*.pdf", docvarsfrom = "filenames", dvsep = "-")
```

## Adding your own slideshow

We encourage you to add your own slideshow in pdf format. To do this, just insert the pdf file in the Input folder and delete any other contents. For demonstration purposes we have included the slideshow you just one, but we think that this demonstration will be more interesting and fun if you can add your own data.

We will then add the text and info on the presentation such as it's title to our presentations data frame. This is straightforward as readtext will give us the same variable names for both imports. We also add an indicator variable which tells us which presentation is the 'extra' one we added. This will be important later for some of the analysis we want to do.

```{r}
your_presentation_df = readtext("./Import/*.pdf", docvarsfrom = "filenames")
if (nrow(your_presentation_df) > 1) stop("More than one pdf in Import folder, please remove extras") 

your_presentation_df$yourpresentation = c(TRUE)
ids_presentations_df$yourpresentation = rep(FALSE,rep=nrow(ids_presentations_df))
#These two lines create the variable your presentation and set it to true only for the presentation imported from the Import folder

yourpresentationname = as.character(as.data.frame(your_presentation_df)[[1]])
#this line saves the name of your specific presentation for later use

presentations_df = rbind(ids_presentations_df, your_presentation_df)
print(presentations_df, n = 10)
```

Here we can see the structure of our dataframe. Luckily for us, readtext automatically formats the dataframe as quantedata expects them, but for future reference if you are constructing your own dataframes for analysis, the text for analysis for each row (which will be each unique text, presentation, section or any other unit of analysis) must be in a column called 'text'.

## Creating a  Corpus

We can now create our first Corpus - this is the object at the centre of the quanteda package.  

```{r}
presentations_corpus = corpus(presentations_df)
summary(presentations_corpus)
```

Here we see the corpus structure with some information about the text in each slideshow, plus the document-level variables we added earlier.

# Preparing our texts for non-relational analysis - Tokens and the DFM

Now that we have our corpus we can begin to analyse the text data. For this section we will focus on non-relational analysis, i.e. we will look at the presence and frequency of words in each slideshow, regardless of their context. The first process our data will undergo is tokenisation. This splits up the text of each document into tokens, which are usually words, but can be other standalone character units.

Once we have tokenised our corpus we can create a Document feature matrix. This (insert information about DFM).

Finally, we run the function top features, to have a look at the most common tokens in the entire corpus. This is our first analysis function, just to get an overview, but we will dive into more later.

Don't worry if this process of tokenisation and constructing a dfm still feels unclear, we will go over it a few times, and you'll have a chance to adjust the process yourselves.

```{r}
presentation_tokens = tokens(presentations_corpus)

presentation_dfm = dfm(presentation_tokens)

print(presentation_dfm)

topfeatures(presentation_dfm)
```

Looking at this, we can see that all the most common tokens by far are punctuation or filler words which don't carry much meaning. This is normal, and luckily quantedata comes built-in with some handy functions and options to help alleviate this.

The first thing we can do is adjust the parameters on the function tokens(), allowing us to remove punctuation, numbers and symbols.

An even more powerful tool is the token_remove function combined with the built-in stopwords database. Quantedata comes loaded with common filler words or stop words for _ different languages. Removing these may help us with our analysis.

```{r}
presentation_tokens = tokens(presentations_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)

presentation_tokens = tokens_remove(presentation_tokens, stopwords("en"))

presentation_dfm = dfm(presentation_tokens)

topfeatures(presentation_dfm)
```
Now this looks more promising!

You can use your own discretion to decide whether punctuation, numbers, symbols or stop words should be included or excluded in your use case. It is also possible to remove words individually. For example, I felt that 'can' and 'use' were not words that carried much useful meaning, especially with their very high frequency in the data. Have a look at the outputs above and see if there are any adjustments you would make for our final tokenisation process.

(We repeat the whole process here for the sake of having it all in once place as an overview)

```{r}
presentation_tokens = tokens(presentations_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,remove_url = TRUE, remove_separators = TRUE, split_hyphens = FALSE, split_tags = FALSE)#Do you think we should be excluding all these categories? Feel free to change these options as you feel makes the most sense

presentation_tokens = tokens_remove(presentation_tokens, stopwords("en"))
presentation_tokens = tokens_remove(presentation_tokens, c('can','use')) #You can add any 'filler' words here which you would like excluded from the analysis.

presentation_dfm = dfm(presentation_tokens)

topfeatures(presentation_dfm)
```

Now we have a nice looking Document Feature matrix for our further analysis. For our live presentation we will stop here and move on to the next section: Analysing text non-relationally, but if you would like to learn some more about creating and tweaking corpuses continue below.



# Analysing our texts non-relationally

There are many many functions across the quantedata family of packages which can be used to analyse our newly created DFM, here we will focus on some of the fucntions we foudn the most interesting from the textstat and testplot packages.

## Lexical Diversity

The first function we look at is textstat_lexdiv(), which calculates the lexical diverstiy of the text. As a default this is the ratio of unique tokens to total tokens in the text (TTR), but you can also use the (measure = ) parameter to change to measure.

Also important to note here that while the output of most functions in textstat look like data frames, they are initially stored as lists, of specific type depending on the funciton. We use as.list(), or as.data.frame() to tranform them into more useable data types as seen below.

```{r}
typeof(textstat_lexdiv(presentation_dfm))

as.data.frame(textstat_lexdiv(presentation_dfm)) %>%
  arrange(desc(TTR)) %>% gt() #ordering our results in terms of TTR and presenting them in a nice table
```

## Distance and Similarity

Another interesting function for looking at the distance between two text documents is textstat_dist(), which can calculate the distance between two documents, which can most easily be understood as how different the choice and frequency of words used is between the documents. 

```{r}
as.data.frame(textstat_dist(presentation_dfm)) %>% #Here we run the function, with the default, euclidean distance measurement.
  
  filter(document2 == yourpresentationname) %>% arrange(euclidean) %>% select (c(1,3)) %>% gt() #Here we use dplyr to filter for only the distances relative to your presentation, arrange in ascending order of distance and filter out the column of your presentation name repeated and gt to give us a nice readable table
```

Which IDS slideshow is your presentation most like?

Another similar function is textstat_simil() which calculates similarities, and defaults to the correlation in two documents. See if you can make a similar function to the one above and see how well the results match.

```{r}
#Insert your code here:
```

## Grouping DFMs

Above, we filtered out all comparisons not involving your presentation manually, but because our document-level variables have been retained in our DFM, we can actually group the presentation by the variable 'yourpresentation' we created earlier. (Remember, this variable is true for the presentation you inputted and false for all others.)

One way of doing this is the dfm_group() function, which is useful as part of a pipe, to pass the group DFMs based on a parameter of interest into the next function.

Some functions in the package have funcitonality for this built in, as we can see with topfeatures below.

```{r}
dfm_group(presentation_dfm, presentation_dfm$yourpresentation)

topfeatures(groups = yourpresentation)
```

While this is just a toy example as our documents don't have many document level variables, there are many useful applications of this in analysing texts, like grouping by chapter in a book, by which party gave a political speech, or any other category of interest in our documents.


## Word Clouds

Just for fun: lets make an ever popular word cloud! We can do this super easily with the textplot_wordcloud() function.
```{r}
textplot_wordcloud(presentation_dfm, max_words = 50)

dfm_group(presentation_dfm, presentation_dfm$yourpresentation) %>% textplot_wordcloud( max_words = 20, comparison = TRUE)
```


# Natural Language Processing

