---
author: "Killian, Luca & Aranxa"
title: "Quanteda"
subtitle: "Quantitative Text Analyis"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Welcome!üëã

This Workshop introduces you to quanteda, an R package for quantitative text analysis.

# Text Analysis üìö

Why do we want to analyze textual data? Text is everywhere, grows in volume and becomes more easily accessible as we speak. Main reasons for this are the widespread use of social media and the digitalisation of many aspects of our lives, making lots of data available for (social science) research. Natural language processing provides us with the fundamental tools and methods to analyse text data.

### Natural Language Processing

Natural Language Processing (NLP) is concerned with enabling computers/machines to process, understand, interpret and even generate natural language, for example in the form of text or speech. Some examples of applications using NLP are chatbots, tools for speech recognition, automatic text summarization or translation, such as Google Translate.

### Quantitative Text Analysis
<<<<<<< HEAD
Quantitative text analysis is a subfield of NLP and refers to the process of analyzing
text data using statistical and computational methods to derive quantitative information
from the input text. Some examples of quantitative text analysis include word frequency
analysis, keyword extraction, sentiment analysis, text visualization, and more. All of this can be done using the R package quanteda!
=======

### Use Cases

Some popular use cases of quantitative text analysis in social science research are: 
- The analysis of political speeches or party manifestos to identify key phrases 
- Social Media Listening
- Categorization and quantification of open-ended survey responses. 
In order to illustrate the workflow and main functions of the quanteda package, we analyse speeches by Barack Obama which we previously scraped from http://obamaspeeches.com/.

There's a great literature regarding discourse analysis of political speeches. For instance you can check at a very brief example from Sharififar, M., & Rahimi, E. (2015). Critical discourse analysis of political speeches: A case study of Obama's and Rouhani's speeches at UN. Theory and Practice in Language studies, 5(2), 343. https://www.academypublication.com/issues2/tpls/vol05/02/14.pdf

Our main goal is for you to learn the basic functions that will allow you to achieve this type of analysis in R!

# Quanteda üí°

### Background
<<<<<<< HEAD
Quanteda is an R packagae for managing and analysing textual data. It was created by
[Ken Benoit](https://kenbenoit.net) and is available via [CRAN](https://cran.r-
project.org/web/packages/quanteda/index.html). The latest update of quanteda is version 4.0 and the package is maintained by the UK-based non-profit company [Quanteda
Initiative](https://quanteda.org).
=======
>>>>>>> 32964ae60c83dacc415cc180b667c01accaf08cd

### Background

Quanteda is an R packagae for managing and analysing textual data. It was created by [Ken Benoit](https://kenbenoit.net) and is available via [CRAN](https://cran.r-%20project.org/web/packages/quanteda/index.html). The latest update of quanteda is version 4.0 and the package is maintained by the UK-based non-profit company [Quanteda Initiative](https://quanteda.org).

Since Version 3.0, quanteda has been split into modular packages. These consist of: 

**quanteda:** - Contains all core natural language processing and textual data management functions

**quanteda.textmodels:** - Contains all text models and supporting functions - Textmodel\_\* functions

**quanteda.textstats:** - Statistics for textual data - textstat\_\*() functions

**quanteda.textplots:** - Plots for textual data - textplot\_\*() functions

<<<<<<< HEAD
Additionally available via GitHub:

**quanteda.sentiment:**
-   Sentiment analysis using dictionaries
=======
Additionally available via GitHub: **quanteda.sentiment:** - Sentiment analysis using dictionaries
>>>>>>> 32964ae60c83dacc415cc180b667c01accaf08cd

**quanteda.tidy:** - Extensions for manipulating document variables in core quanteda objects using tidyverse functions

# Quanteda Basics üìù

The quanteda workflow is structured around three main objects, these are

<<<<<<< HEAD
1. the Corpus
2. Tokens
3. The Document-Feature Matrix (dfm)
=======
1.  the corpus
2.  tokens
3.  the document-feature matrix (dfm)
>>>>>>> 32964ae60c83dacc415cc180b667c01accaf08cd

## Corpus
In quanteda, a corpus functions as the primary data structure for storing and organizing text data. The corpus stores 

The corpus can be thought of as "library" or data frame that holds the textual data we want to analyse alongside specific document-level or metadata attributes, called docvars. Only by transforming our input text into a corpus format, quanteda is able to process it, therefore the corpus functions as starting point to our analysis. 

In general, the corpus functions as relatively static library, meaning that text data in the corpus should not be changed through cleaning and preprocessing steps. Rather, texts can be extracted from the corpus for any manipulation and should be assigned to new objects. This way, the corpus functions as a copy of the original input data. Within the corpus, each document is typically represented as a separate element. Documents within a corpus can be accessed by their index or via docvars.


## Tokens
<<<<<<< HEAD
In quanteda, tokens represent the basic units of text data, that are used for any further analysis. They comprise usually of words that have been extracted and grouped as semantic units. Nonetheless, phrases or even entire sentences can be tokenized as well.

Tokenizing describes the process of splitting the input text into individual tokens. Once a text is tokenized, various operations for preprocessing, cleaning and feature extraction can be performed on those tokens.

**How do tokens relate to the Corpus?**
When you tokenize text, you create a list of tokens for each document, and each document in the corpus is represented as a sequence of tokens. Thereby, tokens can be used for positional (string-of-words) analysis, which is used to examine, eg.: the position and relation of tokens within a corpus.

## Document Feature Matrix
The document feature matrix puts documents into matrix format and constitutes the unit on which analyses will be performed. In the DFM, each row corresponds to a document and each column corresponds to a feature of that text, which are often tokens. The matrix stores the frequency of each feature (column) within each document (row). DFMs are particularly useful if not all parts of the documents in the corpus should be included in the analysis.Generally, DFMs are often sparse, meaning that many entries in the matrix are zero, making the DFM more memory- and computationally efficient.

**How does the DFM relate to tokens?**
When you create DFM, the list of tokens is transformed into a structure matrix that functions as foundation for subsequent analysis. Tokens constitute individual elements of text data, while a DFM constitutes a structural and numerical representation of text data. As opposed to tokens, the DFM does not store information on the position of words within a given document. A DFM is therefore used to perform non-positional (bag-of-words) analysis, for example word frequency analysis,sentiment analysis, text classification, and more. 

=======

# Workflow üí™

## Getting started

First we load the core quanteda package.

```{r}
#install.packages('quanteda')
#library(quanteda)
```

Additionally, we load the [readtext package](https://readtext.quanteda.io)

```{r}
#install.packages('readtext')
#library(readtext)
```

The readtext package provides an easy way to read text data into R, from almost any input format. Compatible input formats include .txt, .csv, .tab, .json, .pdf, .doc and .docx files

## Read Data into R

# Webscraping example 

## Try it out! Comparing political discourses
Now we're going to give you an example where you can apply this for one of the most used cases: analyzing political discourses.

**Step 1**. Scraping a discourse
To practice you can use the http://obamaspeeches.com website to explore on his best* speeches(according to the webpage creators). Let's try with his inaugural speech.

```{r}
#Save the link as object
obama_speeches <- "http://obamaspeeches.com/P-Obama-Inaugural-Speech-Inauguration.htm"
obama_inaugural <- read_html(obama_speeches)

```

With Selector Gadget we identify the structure in the html containing the text. (Copy it direct from the bar, do not click on the xpath function.) 
```{r}
inaugural_speech_container <- obama_inaugural |>  html_nodes("br+ table font+ font")

# Extract the text from the container
inaugural_speech_text <- html_text(inaugural_speech_container)

# Print the speech text
cat(inaugural_speech_text, sep = "\n")

```

Here you can have a look at the different speeches you would like to work on later that are available on the website.Just take away the "#" symbol.
```{r}

# titles_speeches <- obama_inaugural |> 
  # html_nodes("table p")

# titles_speeches <- sapply(titles_speeches, html_text, USE.NAMES = FALSE)

# cat(titles_speeches , sep = "\n")
```

Following on what you just learned, you can now explore with the 'corpus' and the 'token' functions how this speech is structured.
```{r}

inaugural_df <- rbind(inaugural_speech_text)
inaugural_speech_corpus <- corpus(inaugural_df )
inaugural_speech_tokens <- tokens(inaugural_speech_corpus)

summary(inaugural_speech_corpus)

```

Transform the speech into a document-feature matrix (dmf)
```{r}

inaugural_speech_dfm <- dfm(inaugural_speech_tokens)

print(inaugural_speech_dfm)

```

Explore which are the most frequend words in the discoure.
```{r}
topfeatures(inaugural_speech_dfm )
```

To keep on working, let's clean the speech a little.
```{r}

inaugural_speech_tokens <- tokens(inaugural_speech_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)

inaugural_speech_tokens <- tokens_remove(inaugural_speech_tokens, stopwords("en"))
inaugural_speech_tokens  <-  tokens_remove(inaugural_speech_tokens, c('the','and', 'that','to', 'can', 'must', 'of', 'every', 'words', 'let', 'end', 'whether'))

inaugural_speech_dfm <- dfm(inaugural_speech_tokens)

topfeatures(inaugural_speech_dfm)
```

Let's see the wordcloud from this speech
```{r}
textplot_wordcloud(inaugural_speech_dfm, max_words = 50)

```
**Step 2**
We'll compare it with another speech, this time with "An Honest Government - A Hopeful Future" which Obama hold at the University of Nairobi under the topic: Our Past, Our Future & Vision for America (August 28, 2006).

```{r}
#Save the link as object
obama_speeches_2 <- "http://obamaspeeches.com/088-An-Honest-Government-A-Hopeful-Future-Obama-Speech.htm"
obama_honest_gov <- read_html(obama_speeches_2)

```

With Selector Gadget we identify the structure in the html containing the text. Copy it direct from the bar, do not click on the xpath function. 
```{r}
honest_gov_speech_container <- obama_honest_gov |>  html_nodes("font p font")

# Extract the text from the container
honest_gov_speech_text <- html_text(honest_gov_speech_container)

# Print the speech text
cat(honest_gov_speech_text, sep = "\n")

```


Following on what you just learned, you can now explore with the 'corpus' and the 'token' functions how this speech is structured.
```{r}

honest_gov_df <- rbind(honest_gov_speech_text)
honest_gov_speech_corpus <- corpus(honest_gov_df)
honest_gov_speech_tokens <- tokens(honest_gov_speech_corpus)

summary(honest_gov_speech_corpus)

```

Transform the speech into a document-feature matrix (dmf)
```{r}

honest_gov_speech_dfm <- dfm(honest_gov_speech_tokens)

print(honest_gov_speech_dfm)

```
Explore which are the most frequend words in the discoure.
```{r}
topfeatures(honest_gov_speech_dfm )
```

To keep on working, let's clean the speech a little.
```{r}

honest_gov_speech_tokens <- tokens(honest_gov_speech_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)

honest_gov_speech_tokens <- tokens_remove(honest_gov_speech_tokens, stopwords("en"))
honest_gov_speech_tokens  <-  tokens_remove(honest_gov_speech_tokens, c('the','and', 'that','to', 'can', 'must', 'of', 'every', 'words', 'let', 'end', 'whether', 'kenya', 'kenyan', 'kenyans', 'also', 'sometimes'))

# We removed "Kenya" and "kenyan" to focus more on the topics around it, since we already know that's the main topic. We want to know what are other concepts associated to it. 

honest_gov_speech_dfm <- dfm(honest_gov_speech_tokens)

topfeatures(honest_gov_speech_dfm)
```

Let's see the Word Cloud from this speech
```{r}
textplot_wordcloud(honest_gov_speech_dfm, max_words = 40)
```

The following functions will be relevant for the live coding session:

construct a corpus from a character vector: `{r}corpus()`

It is also possible to explore the corpus: `{r}summary(corpus_name)`

Extract or add document-level variables: `{r}docvars()`

Other relevant corpus functions include: #Discuss: Do we want to taylor the example functions to our use case or would that take too much time now?

Subset corpora: `{r}corpus_subset()` `{r}corpus_subset(x, Year > 1990)`

Change units of a corpus: `{r}corpus_reshape()` `{r}corpus_reshape(x, to = c("sentences", "paragraphs"))`

Segment texts on a pattern match: `{r}corpus_segment()` `{r}corpus_segment(x, pattern, valuetype, extract_pattern = TRUE)`

Take a random sample of the corpus texts: `{r}corpus_sample` `{r}corpus_sample(x, size = 10, replace = FALSE)`

## Preprocessing

### Cleaning and Creating Tokens

#TO DO: Add short explanations what these functions do --\> Luca #TO DO: Add short explanation based on our use case how we clean + tokenize the data --\> Aranxa

**Main Token functions**

`{r}Tokens()`

`{r}Tokens_tolower()/tokens_toupper()`

`{r}Tokens_wordstem()`

`{r}Tokens_compound()`

`{r}Tokens_lookup()`

`{r}Tokens_ngrams()`

`{r}Tokens_skipgrams()`

`{r}Tokens_select()/tokens_remove()/tokens_keep()/tokens_replace()`

`{r}Tokens_sample()`

`{r}Tokens_subset()`

#TO DO: Should we add other cleaning functions here?

## Document Feature Matrix

#add our use case

`dfm()`

`dictionary()`

`dfm_lookup()`

`dfm_select()`

`dfm_compress()`

`dfm_sample()`

`dfm_weight()`

`dfm_sort()`

`dfm_group()`

## Analyis

### Statistics

### Models

### Plots

>>>>>>> 32964ae60c83dacc415cc180b667c01accaf08cd
# Why quanteda? ‚úÖ

There are several advantages to using quanteda for quantitative text analysis!

-   Quanteda is compatible with other R packages, for example the magrittr \|\> and most of the grammar of the tidyverse!
-   Quanteda is a well maintained package: The quanteda initiative offers user, technical and development support as well as teaching and workshop opportunities
-   It is a relatively easy to use for beginners but offers complex functions for more advanced analyses too
-   And it is fast: Compared to other R and Python package for processinf large textual data, quanteda is faster and more efficient

...And now it's time to try it yourself!

<<<<<<< HEAD
---
# Workflow üí™
=======
------------------------------------------------------------------------

# Loading the package

If
>>>>>>> 32964ae60c83dacc415cc180b667c01accaf08cd

# Loading the package
First we load the core of the quanteda package.

If any of the packages are not installed on your machine you can uncomment the first line.

```{r initialise, warning=FALSE}
#install.packages('quanteda','quanteda.textstats','readtext','gt')

library(quanteda)
```

As mentioned above, the quanteda package is split in to modular packages so we also download a selection of those of interest to us in this tutorial.

```{r initialise2, warning=FALSE}
library(quanteda.textstats)
library(quanteda.textplots)
```

We also load readtext which makes the process of importing texts easier, and also imports meta data. Specifically in our case, it allows us to read in pdfs as text files.
Other compatible input formats include .txt, .csv, .tab, .json, .doc and
.docx files.

```{r initialise3, warning=FALSE}
library(readtext)
```

Finally we load dplyr for our workflow and gt for creating nice looking tables. 

```{r initialise4, warning=FALSE}
library(dplyr)
library(gt)
library(rvest)
```

# Loading and prepping our data - The Corpus

For the example in this tutorial we will load in a collection of slides from the Intro to Data Science Course by Simon Munzert. We use the readtext function as mentioned above.

```{r}
ids_presentations_df = readtext("./Slideshows/*.pdf", docvarsfrom = "filenames")
#rt_pdf = readtext("./Slideshows/*.pdf", docvarsfrom = "filenames", dvsep = "-")
```

## Adding your own slideshow

We encourage you to add your own slideshow in pdf format. To do this, just insert the pdf file in the Input folder and delete any other contents. For demonstration purposes we have included the slideshow you just one, but we think that this demonstration will be more interesting and fun if you can add your own data.

We will then add the text and info on the presentation such as it's title to our presentations data frame. This is straightforward as readtext will give us the same variable names for both imports. We also add an indicator variable which tells us which presentation is the 'extra' one we added. This will be important later for some of the analysis we want to do.

```{r}
your_presentation_df = readtext("./Import/*.pdf", docvarsfrom = "filenames")
if (nrow(your_presentation_df) > 1) stop("More than one pdf in Import folder, please remove extras") 

your_presentation_df$yourpresentation = c(TRUE)
ids_presentations_df$yourpresentation = rep(FALSE,rep=nrow(ids_presentations_df))
#These two lines create the variable your presentation and set it to true only for the presentation imported from the Import folder

yourpresentationname = as.character(as.data.frame(your_presentation_df)[[1]])
#this line saves the name of your specific presentation for later use

presentations_df = rbind(ids_presentations_df, your_presentation_df)
print(presentations_df, n = 10)
```

Here we can see the structure of our dataframe. Luckily for us, readtext automatically formats the dataframe as quantedata expects them, but for future reference if you are constructing your own dataframes for analysis, the text for analysis for each row (which will be each unique text, presentation, section or any other unit of analysis) must be in a column called 'text'.

## Creating a Corpus

We can now create our first Corpus - this is the object at the centre of the quanteda package.

```{r}
presentations_corpus = corpus(presentations_df)
summary(presentations_corpus)
```

Here we see the corpus structure with some information about the text in each slideshow, plus the document-level variables we added earlier.

# Preparing our texts for non-relational analysis - Tokens and the DFM

Now that we have our corpus we can begin to analyse the text data. For this section we will focus on non-relational analysis, i.e. we will look at the presence and frequency of words in each slideshow, regardless of their context. The first process our data will undergo is tokenization. This splits up the text of each document into tokens, which are usually words, but can be other standalone character units.

Once we have tokenized our corpus we can create a Document feature matrix as introduced above. We introduce these concepts these concepts together as that is how you will usually emply the min the wild. But there are also some use cases where you might stop after tokenization.

Finally, we run the function top features, to have a look at the most common tokens in the entire corpus. This is our first analysis function, just to get an overview, but we will dive into more later.

Don't worry if this process of tokenization and constructing a dfm still feels unclear, we will go over it a few times, and you'll have a chance to adjust the process yourselves.

```{r}
presentation_tokens = tokens(presentations_corpus)

presentation_dfm = dfm(presentation_tokens)

print(presentation_dfm)

topfeatures(presentation_dfm)
```

There we have it, our first DFM! Not so difficult to begin with.

Looking at the top features, we can see that all the most common tokens by far are punctuation or filler words which don't carry much meaning. This is normal, and luckily quantedata comes built-in with some handy functions and options to help alleviate this.

## Cleaning our Tokens

The first thing we can do is adjust the parameters on the function tokens(), allowing us to remove punctuation, numbers and symbols.

An even more powerful tool is the token_remove function combined with the built-in stopwords database. Quantedata comes loaded with common filler words or stop words for \_ different languages. Removing these may help us with our analysis.

```{r}
presentation_tokens = tokens(presentations_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)

presentation_tokens = tokens_remove(presentation_tokens, stopwords("en"))

presentation_dfm = dfm(presentation_tokens)

topfeatures(presentation_dfm)
```

Now this looks more promising!

### Bringing it all together

You can use your own discretion to decide what exactly should be kept or excluded from your tokens. For example, I felt that 'can' and 'use' were not words that carried much useful meaning, especially with their very high frequency in the data, so I choose to remove the mbelow. Have a look at the outputs above and see if there are any adjustments you would make for our final tokenisation process, then apply them below.

We have included some functions for you already here and there are even more in the section entitled: More Token Cleaunup, whcih you can add to the pipeline.

(We repeat the whole process here for the sake of having it all in once place as an overview)

```{r}
presentation_tokens = tokens(presentations_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,remove_url = TRUE, remove_separators = TRUE, split_hyphens = FALSE, split_tags = FALSE)#Do you think we should be excluding all these categories? Feel free to change these options as you feel makes the most sense

presentation_dfm = presentation_tokens %>% 
  tokens_remove(pattern= stopwords("en")) %>% #removing the stopwords as before
  tokens_remove(pattern = c('can','use')) %>% #Removing can and use as mentioned, you can add any 'filler' words which you would like excluded from the analysis here.
  #Some more functions you may want to consider for cleaning your tokens (uncomment first line for each to use and remember to include pipe symbol at the end):
  #tokens_tolower() %>%
  #Changes all tokens to be lower case
  #tokens_toupper() %>%
  #Changes all tokens to be upper case
  #tokens_replace(pattern = c('color'), replacement = ('colour')) %>%
  #Allows you to find and replace words in your tokens, remember that pattern and replacement should always be the same length and that entries are matched pairwise.
  dfm() #Finally we run the dfm function on our finished and cleaned tokens.

topfeatures(presentation_dfm)
```

Now we have a nice looking Document Feature matrix for our further analysis. For our live presentation we will stop here and move on to the next section: Analysing text non-relationally, but if you would like to learn some more about creating and tweaking corpuses continue below.

<<<<<<< HEAD
## Additional Functions for Corpuses

Subset corpora:
```{r}corpus_subset()```
```{r}corpus_subset(x, Year > 1990)```

Change units of a corpus:
```{r}corpus_reshape()```
```{r}corpus_reshape(x, to = c("sentences", "paragraphs"))```

Segment texts on a pattern match:
```{r}corpus_segment()```
```{r}corpus_segment(x, pattern, valuetype, extract_pattern = TRUE)```

Take a random sample of the corpus texts:
```{r}corpus_sample```
```{r}corpus_sample(x, size = 10, replace = FALSE)```

## Additional Funtions for Tokens

### More Cleaning

tokens_keep() - For when we only want to keep tokens matching a certain criteria.

Tokens_select() - The function underlying tokens_keep and tokens_remove. Use the selections = 'keep' or 'remove' parameter to pick what you want to do with the matched tokens.

Tokens_wordstem() - This is a really interesting and useful function that reduces all words and tokens to their root. We use a very simple sentence below to show an example of this in action.

```{r}
tokens_wordstem(tokens('She runs, he can run, they are running'))
```

```{r}Tokens_compound()```

### Other Token functions (From Luca's list, still need doing.)
```{r}Tokens_lookup()```

```{r}Tokens_ngrams()```

```{r}Tokens_skipgrams()```

```{r}Tokens_sample()```

```{r}Tokens_subset()```

### More document Feature Matrix functions
```dictionary()```

```dfm_lookup()```

```dfm_select()```

```dfm_compress()```

```dfm_sample()```

```dfm_weight()```

```dfm_sort()```

=======
>>>>>>> 32964ae60c83dacc415cc180b667c01accaf08cd
# Analysing our texts non-relationally

There are many many functions across the quantedata family of packages which can be used to analyse our newly created DFM, here we will focus on some of the fucntions we foudn the most interesting from the textstat and testplot packages.

## Lexical Diversity

The first function we look at is textstat_lexdiv(), which calculates the lexical diverstiy of the text. As a default this is the ratio of unique tokens to total tokens in the text (TTR), but you can also use the (measure = ) parameter to change to measure.

Also important to note here that while the output of most functions in textstat look like data frames, they are initially stored as lists, of specific type depending on the funciton. We use as.list(), or as.data.frame() to tranform them into more useable data types as seen below.

```{r}
typeof(textstat_lexdiv(presentation_dfm))

as.data.frame(textstat_lexdiv(presentation_dfm)) %>%
  arrange(desc(TTR)) %>% gt() #ordering our results in terms of TTR and presenting them in a nice table
```

## Distance and Similarity

Another interesting function for looking at the distance between two text documents is textstat_dist(), which can calculate the distance between two documents, which can most easily be understood as how different the choice and frequency of words used is between the documents.

```{r}
as.data.frame(textstat_dist(presentation_dfm)) %>% #Here we run the function, with the default, euclidean distance measurement.
  
  filter(document2 == yourpresentationname) %>% arrange(euclidean) %>% select (c(1,3)) %>% gt() #Here we use dplyr to filter for only the distances relative to your presentation, arrange in ascending order of distance and filter out the column of your presentation name repeated and gt to give us a nice readable table
```

Which IDS slideshow is your presentation most like?

Another similar function is textstat_simil() which calculates similarities, and defaults to the correlation in two documents. See if you can make a similar function to the one above and see how well the results match.

```{r}
#Insert your code here:
```

## Grouping DFMs

Above, we filtered out all comparisons not involving your presentation manually, but because our document-level variables have been retained in our DFM, we can actually group the presentation by the variable 'yourpresentation' we created earlier. (Remember, this variable is true for the presentation you inputted and false for all others.)

One way of doing this is the dfm_group() function, which is useful as part of a pipe, to pass the group DFMs based on a parameter of interest into the next function.

Some functions in the package have funcitonality for this built in, as we can see with topfeatures below.

```{r}
dfm_group(presentation_dfm, presentation_dfm$yourpresentation)

topfeatures(presentation_dfm, groups = yourpresentation)
```

While this is just a toy example as our documents don't have many document level variables, there are many useful applications of this in analysing texts, like grouping by chapter in a book, by which party gave a political speech, or any other category of interest in our documents.

## Word Clouds

Just for fun: lets make an ever popular word cloud! We can do this super easily with the textplot_wordcloud() function.

```{r}
textplot_wordcloud(presentation_dfm, max_words = 50)

#dfm_group(presentation_dfm, presentation_dfm$yourpresentation) %>% textplot_wordcloud( max_words = 20, comparison = TRUE) - Can't get working yet - Killian
```

# Natural Language Processing
