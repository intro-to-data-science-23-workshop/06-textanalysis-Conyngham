---
author: "Killian, Luca, Aranxa"
title: "Quanteda"
subtitle: "Analysing text data quantitively"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Insert session Intro here

---
# Loading the package
If 

First we load the core of the quantedata package. If any of the packages are not installed on your machine you can uncomment the first line.

```{r initialise, warning=FALSE}
#install.packages('quanteda','quanteda.textstats','readtext')

library(quanteda)
```

The package is split in to modular packages so we also download a selection of those of interest to us in this tutorial.

```{r initialise2, warning=FALSE}
library(quanteda.textstats)
library(quanteda.textplots)
```

Finally we load readtext which makes the process of importing texts easier, and also imports meta data. Specifically in our case, it allows us to read in pdfs as text files.

```{r initialise3, warning=FALSE}
library(readtext)
```

# Loading and prepping our data - The Corpus

For the example in this tutorial we will load in a collection of slides from the Intro to Data Science Course by Simon Munzert. We use the readtext function as mentioned above.

```{r}
ids_presentations_df = readtext("./Slideshows/*.pdf", docvarsfrom = "filenames")
#rt_pdf = readtext("./Slideshows/*.pdf", docvarsfrom = "filenames", dvsep = "-")
```

## Adding your own slideshow

We encourage you to add your own slideshow in pdf format. To do this, just insert the pdf file in the Input folder and delete any other contents. For demonstration purposes we have included the slideshow you just one, but we think that this demonstration will be more interesting and fun if you can add your own data.

We will then add the text and info on the presentation such as it's title to our presentations data frame. This is straightforward as readtext will give us the same variable names for both imports. We also add an indicator variable which tells us which presentation is the 'extra' one we added. This will be important later for some of the analysis we want to do.

```{r}
your_presentation_df = readtext("./Import/*.pdf", docvarsfrom = "filenames")
if (nrow(your_presentation_df) > 1) stop("More than one pdf in Import folder, please remove extras") 

your_presentation_df$yourpresentation = c(TRUE)
ids_presentations_df$yourpresentation = rep(FALSE,rep=nrow(ids_presentations_df))
#These two lines create the variable your presentation and set it to true only for the presentation imported from the Import folder

presentations_df = rbind(ids_presentations_df, your_presentation_df)
print(presentations_df, n = 10)
```

Here we can see the structure of our dataframe. Luckily for us, readtext automatically formats the dataframe as quantedata expects them, but for furute reference if you are constructing your own dataframes for analysis, the text for analysis for each row (which will be each unique text, presentation, section or any other unit of analysis) must be in a column called 'text'.

## Creating a  Corpus

We can now create our first Corpus - this is the object at the centre of the quanteda package.  

```{r}
presentations_corpus = corpus(presentations_df)
summary(presentations_corpus)
```

Here we see the corpus structure with some information about the text in each slideshow, plus the document-level variables we added earlier.

# Preparing our texts for non-relational analysis - Tokens and the DFM

Now that we have our corpus we can begin to analyse the text data. For this section we will focus on non-relational analysis, i.e. we will look at the presence and frequency of words in each slideshow, regardless of their context. The first process our data will undergo is tokenisation. This splits up the text of each document into tokens, which are usually words, but can be other standalone character units.

Once we have tokenised our corpus we can create a Document feature matrix. This (insert infor about DFM).

Finally, we run the function top features, to have a look at the most common tokens in the entire corpus. This is our first analysis function, just to get an overview, but we will dive into more later.

Don't worry if this process of tokenisation and constructing a dfm still feels unclear, we will go over it a few times, and you'll have a chance to adjust the process yourselves.

```{r}
presentation_tokens = tokens(presentations_corpus)

presentation_dfm = dfm(presentation_tokens)

print(presentation_dfm)

topfeatures(presentation_dfm)
```

Looking at this, we can see that all the most common tokens by far are punctuation or filler words which don't carry much meaning. This is normal, and luckily quantedata comes built-in with some handy functions and options to help alleviate this.

The first thing we can do is adjust the parameters on the function tokens(), allowing us to remove punctuation, numbers and symbols.

An even more powerful tool is the token_remove function combined with the build in stopwords database. Quantedata comes loaded with common filler words or stop words for _ different languages. Removing these may help us with our analysis.

```{r}
presentation_tokens = tokens(presentations_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)

presentation_tokens = tokens_remove(presentation_tokens, stopwords("en"))

presentation_dfm = dfm(presentation_tokens)

topfeatures(presentation_dfm)
```
Now this looks more promising!

You can use your own discretion to decide whether punctuation, numbers, symbols or stop words should be included or excluded in your use case. It is also possible to remove words individually. For example, I felt that 'can' and 'use' were not words that carried much useful meaning, especially with their very high frequency in the data. Have a look at the outputs above and see if there are any adjustments you would make for our final tokenisation process.

(We repeat the whole process here for the sake of having it all in once place as an overview)

```{r}
presentation_tokens = tokens(presentations_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,remove_url = TRUE, remove_separators = TRUE, split_hyphens = FALSE, split_tags = FALSE)#Do you think we should be excluding all these categories? Feel free to change these options as you feel makes the msot sense

presentation_tokens = tokens_remove(presentation_tokens, stopwords("en"))
presentation_tokens = tokens_remove(presentation_tokens, c('can','use')) #You can add any 'filler' words here which you would like excluded from the analysis.

presentation_dfm = dfm(presentation_tokens)

topfeatures(presentation_dfm)
```

Now we have a nice looking Document Feature matrix for our further analysis. For our live presentation we will stop here and move on to the next section: Analysing text non-relationally, but if you would like to learn some more about creating and tweaking corpuses continue below.


# Analysing our texts non-relationally

There are many many functions across the quantedata family of packages which can be used to analyse our newly created DFM, here we will focus on some of the fucntions we foudn the most interesting from the textstat and testplot packages.

```{r}
textstat_lexdiv(presentation_dfm) 
```

```{r}
textstat_dist(presentation_dfm)
```

```{r}
textstat_dist(presentation_dfm)
```


Just for fun: lets make an ever popular word cloud! We can do this super easily with the textplot_wordcloud() function.
```{r}
textplot_wordcloud(presentation_dfm, max_words = 50)

```


#Natural Language Processing
